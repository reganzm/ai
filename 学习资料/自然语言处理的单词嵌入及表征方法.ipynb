{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = requests.get('http://www.mahaixiang.cn/internet/886.html')\n",
    "res.encoding='gbk'\n",
    "soup = BeautifulSoup(res.text,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = soup.select('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "rr = []\n",
    "for r in result:\n",
    "    if r.text.strip():\n",
    "             rr.append(r.text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>在过去的几年中，深度神经网络在模式识别中占据着绝对主流，它们在许多计算机视觉任务中完爆之前的...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>在这篇文章里，我综述一下在自然语言处理（NLP）上应用深度神经网络得到的一些效果极其显著的成...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1、单隐层神经网络</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>单隐层神经网络有一个普适性（universality）：给予足够的隐结点，它可以估算任何函数...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>本质上这个理论是正确的，因为隐层可以用来做查询表。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>简单点，我们来看一个感知器网络（perceptron network），感知器 （perce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>注意可能的输入个数是有限的，对每个可能的输入，我们可以在隐层里面构建一个只对这个输入有反应的...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>然后我们可以利用这个神经元和输出神经元之间的连接来控制这个输入下得到的结果（马海祥博客注解：...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>这样可以说明单隐层神经网络的确是有普适性的，但是这也没啥了不起的呀，你的模型能干和查询表一样...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>在马海祥看来，普适性的真正意义是：一个网络能适应任何你给它的训练数据，这并不代表插入新的数据...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>所以，普适性并不能解释为什么神经网络如此好用，真正的原因比这微妙得多，为了理解它，我们需要先...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2、单词嵌入（Word Embeddings）</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>我想从深度学习研究的一个非常有意思的部分讲起，它就是：单词嵌入（word embeddings）。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>在我看来，单词嵌入是目前深度学习最让人兴奋的领域之一，尽管它最早是由Bengio等人在十多年...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>单词嵌入W:words→Rn是一个参数化函数，它把某个语言里的单词映射成高维向量（大概200...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>W(‘‘cat”)=(0.2, -0.4, 0.7, …)\\r\\n\\t\\t　　W(‘‘mat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>一般这个函数就是一个查询表，用一个矩阵θ来参数化，每行是一个单词：Wθ(wn)=θn.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>初始化时，W中每个词对应一个随机的向量，它会学习出有意义的向量以便执行任务。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>举个一个可能的任务的例子：训练一个网络让其预测一个5元组（5-gram）（连续的5个词）是否...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>我们训练的模型会通过W把5元组中每个词的表征向量取出来，输入给另外一个叫R的模块，模块R会试...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>R(W(‘‘cat”), W(‘‘sat”), W(‘‘on”), W(‘‘the”), W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>为了准确地预测这些值，这个网络需要从W以及R中学习到好的参数。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>现在看来这个任务并没什么意思，也许它能用来检测语法错误什么的，没什么大不了，但是极其有趣的部...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>事实上，对我们来说，这个任务的意义就是学习W，我们当然也可以做一些其他的任务，一个很常见的任...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>想直观感受一下单词嵌入空间的话，我们可以用t-SNE来对它进行可视化，t-SNE是一个复杂的...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>t-SNE对单词嵌入的可视化结果，左图：数字区间；右图：工作岗位区间</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>这种单词构成的“地图”对我们来说更直观，相似的词离得近，另一种方法是看对一个给定单词来说，哪...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>网络能让意义相似的词拥有相似的向量，这看起来是很自然的事，如果你把一个词换成它的同义词(例如...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>虽然从字面上看，句子变化很大，但如果W把同义词（像“few”和”couple”这种）映射到相...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>可能的5元组的数目是巨大的，相比之下我们的训练数据量很小，相似的单词距离近能让我们从一个句子...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>基本思路就是你可以通过单词嵌入输出的向量来对图像进行分类，狗的图像会被映射到“狗”的单词向量...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>有趣的是如果你用新类别的图像来测试这个模型会发生什么呢？比如，如果这个模型没训练过如何分类“...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>结果表明，这个网络是可以很合理地处理新类别的图像的，猫的图片并没有被映射到单词嵌入空间的随机...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>相反的，他们更倾向于被映射到整体上相近的“狗”的向量中去，并且事实上更接近于“猫”的向量，相...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>这个图是斯坦福一个小组用8个已知类（和2个未知类别）做的图，结果已经很可观了，但因为已知类数...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>差不多同时期，Google的小组做了一个大得多的版本，他们用了1000个类别而不是8个，之后...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>他们的成果是很赞的，虽然他们不能把未知类的图片准确放到代表这个类的向量上去，但是他们能够把它...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>即使我从来没见过艾斯库拉普蛇和穿山甲，如果你给我看这两样东西的照片，我能告诉你哪个是哪个因为...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>在马海祥看来，共享嵌入是一个非常让人兴奋的研究领域，它暗示着为何深度学习中这个注重表征方法的...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>4、递归神经网络</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>我们之前是用下面这个网络开始谈单词嵌入的：</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>学习单词嵌入的模块化网络</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>上面的图描绘了一个模块化网络，R(W(w1), W(w2), W(w3), W(w4), W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>像上面那样的模型很有效，但很不幸它们有个局限：输入参数的个数必须是固定的。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>我们可以通过加入一个关联模块A来解决这个问题，这个关联模块可以将两个单词或词组的表征合并起来。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>通过合并一系列的单词，A让我们不仅能够表示单词，而且能够表示词组甚至整个句子！另外因为我们可...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>把句子中的单词线性地合并在一起的做法并不是在所有情况下都讲得通，考虑下面这个句子“the c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>这样的模型通常被称作“递归神经网络”因为一个模块经常会使用另外一个同类型模块的输出，有时候它...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>递归神经网络在一系列NLP任务中都有很重大的成功，比如Socher et al. (2013...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>一直以来，一个很主要的目标是如何创建一个可逆的句子表征（sentence represent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>如果这个能成功，将会是一个极其强大的工具，举个例子，我们可以尝试做一个双语句子表征然后把它用...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>不幸的是，这个实际上是很难实现的，非常，非常难，同时因为它一旦成功有巨大的前途，有很多人在为...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>最近，Cho et al. (2014)在词组表征上有了一些进展，他们做了一个能把英语词组编...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>词组表征的t-SNE的一小部分</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>有关上面我们综述的一些结果，我也听说有其他领域的研究人员，尤其是NLP和语言学的人，对他们进...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>马海祥博客点评：</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>深度学习中的表征视角是非常有力的，也似乎能够解答为何深度神经网络如此有效，在此之上，我认为它...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>深度学习是个非常年轻的领域，理论根基还不强，观点也在快速地改变，我感觉神经网络中重视表征的这...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>相关热词搜索：\\n自然语言\\n语言处理\\n单词嵌入\\n表征方法</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>上一篇：九大排序算法的实现方法及算法分析\\n下一篇：详解内存数据库中的索引技术</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>87 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    0\n",
       "0   在过去的几年中，深度神经网络在模式识别中占据着绝对主流，它们在许多计算机视觉任务中完爆之前的...\n",
       "1   在这篇文章里，我综述一下在自然语言处理（NLP）上应用深度神经网络得到的一些效果极其显著的成...\n",
       "2                                           1、单隐层神经网络\n",
       "3   单隐层神经网络有一个普适性（universality）：给予足够的隐结点，它可以估算任何函数...\n",
       "4                           本质上这个理论是正确的，因为隐层可以用来做查询表。\n",
       "5   简单点，我们来看一个感知器网络（perceptron network），感知器 （perce...\n",
       "6   注意可能的输入个数是有限的，对每个可能的输入，我们可以在隐层里面构建一个只对这个输入有反应的...\n",
       "7   然后我们可以利用这个神经元和输出神经元之间的连接来控制这个输入下得到的结果（马海祥博客注解：...\n",
       "8   这样可以说明单隐层神经网络的确是有普适性的，但是这也没啥了不起的呀，你的模型能干和查询表一样...\n",
       "9   在马海祥看来，普适性的真正意义是：一个网络能适应任何你给它的训练数据，这并不代表插入新的数据...\n",
       "10  所以，普适性并不能解释为什么神经网络如此好用，真正的原因比这微妙得多，为了理解它，我们需要先...\n",
       "11                            2、单词嵌入（Word Embeddings）\n",
       "12  我想从深度学习研究的一个非常有意思的部分讲起，它就是：单词嵌入（word embeddings）。\n",
       "13  在我看来，单词嵌入是目前深度学习最让人兴奋的领域之一，尽管它最早是由Bengio等人在十多年...\n",
       "14  单词嵌入W:words→Rn是一个参数化函数，它把某个语言里的单词映射成高维向量（大概200...\n",
       "15  W(‘‘cat”)=(0.2, -0.4, 0.7, …)\\r\\n\\t\\t　　W(‘‘mat...\n",
       "16        一般这个函数就是一个查询表，用一个矩阵θ来参数化，每行是一个单词：Wθ(wn)=θn.\n",
       "17             初始化时，W中每个词对应一个随机的向量，它会学习出有意义的向量以便执行任务。\n",
       "18  举个一个可能的任务的例子：训练一个网络让其预测一个5元组（5-gram）（连续的5个词）是否...\n",
       "19  我们训练的模型会通过W把5元组中每个词的表征向量取出来，输入给另外一个叫R的模块，模块R会试...\n",
       "20  R(W(‘‘cat”), W(‘‘sat”), W(‘‘on”), W(‘‘the”), W...\n",
       "21                    为了准确地预测这些值，这个网络需要从W以及R中学习到好的参数。\n",
       "22  现在看来这个任务并没什么意思，也许它能用来检测语法错误什么的，没什么大不了，但是极其有趣的部...\n",
       "23  事实上，对我们来说，这个任务的意义就是学习W，我们当然也可以做一些其他的任务，一个很常见的任...\n",
       "24  想直观感受一下单词嵌入空间的话，我们可以用t-SNE来对它进行可视化，t-SNE是一个复杂的...\n",
       "25                 t-SNE对单词嵌入的可视化结果，左图：数字区间；右图：工作岗位区间\n",
       "26  这种单词构成的“地图”对我们来说更直观，相似的词离得近，另一种方法是看对一个给定单词来说，哪...\n",
       "27  网络能让意义相似的词拥有相似的向量，这看起来是很自然的事，如果你把一个词换成它的同义词(例如...\n",
       "28  虽然从字面上看，句子变化很大，但如果W把同义词（像“few”和”couple”这种）映射到相...\n",
       "29  可能的5元组的数目是巨大的，相比之下我们的训练数据量很小，相似的单词距离近能让我们从一个句子...\n",
       "..                                                ...\n",
       "57  基本思路就是你可以通过单词嵌入输出的向量来对图像进行分类，狗的图像会被映射到“狗”的单词向量...\n",
       "58  有趣的是如果你用新类别的图像来测试这个模型会发生什么呢？比如，如果这个模型没训练过如何分类“...\n",
       "59  结果表明，这个网络是可以很合理地处理新类别的图像的，猫的图片并没有被映射到单词嵌入空间的随机...\n",
       "60  相反的，他们更倾向于被映射到整体上相近的“狗”的向量中去，并且事实上更接近于“猫”的向量，相...\n",
       "61  这个图是斯坦福一个小组用8个已知类（和2个未知类别）做的图，结果已经很可观了，但因为已知类数...\n",
       "62  差不多同时期，Google的小组做了一个大得多的版本，他们用了1000个类别而不是8个，之后...\n",
       "63  他们的成果是很赞的，虽然他们不能把未知类的图片准确放到代表这个类的向量上去，但是他们能够把它...\n",
       "64  即使我从来没见过艾斯库拉普蛇和穿山甲，如果你给我看这两样东西的照片，我能告诉你哪个是哪个因为...\n",
       "65  在马海祥看来，共享嵌入是一个非常让人兴奋的研究领域，它暗示着为何深度学习中这个注重表征方法的...\n",
       "66                                           4、递归神经网络\n",
       "67                              我们之前是用下面这个网络开始谈单词嵌入的：\n",
       "68                                       学习单词嵌入的模块化网络\n",
       "69  上面的图描绘了一个模块化网络，R(W(w1), W(w2), W(w3), W(w4), W...\n",
       "70              像上面那样的模型很有效，但很不幸它们有个局限：输入参数的个数必须是固定的。\n",
       "71    我们可以通过加入一个关联模块A来解决这个问题，这个关联模块可以将两个单词或词组的表征合并起来。\n",
       "72  通过合并一系列的单词，A让我们不仅能够表示单词，而且能够表示词组甚至整个句子！另外因为我们可...\n",
       "73  把句子中的单词线性地合并在一起的做法并不是在所有情况下都讲得通，考虑下面这个句子“the c...\n",
       "74  这样的模型通常被称作“递归神经网络”因为一个模块经常会使用另外一个同类型模块的输出，有时候它...\n",
       "75  递归神经网络在一系列NLP任务中都有很重大的成功，比如Socher et al. (2013...\n",
       "76  一直以来，一个很主要的目标是如何创建一个可逆的句子表征（sentence represent...\n",
       "77  如果这个能成功，将会是一个极其强大的工具，举个例子，我们可以尝试做一个双语句子表征然后把它用...\n",
       "78  不幸的是，这个实际上是很难实现的，非常，非常难，同时因为它一旦成功有巨大的前途，有很多人在为...\n",
       "79  最近，Cho et al. (2014)在词组表征上有了一些进展，他们做了一个能把英语词组编...\n",
       "80                                    词组表征的t-SNE的一小部分\n",
       "81  有关上面我们综述的一些结果，我也听说有其他领域的研究人员，尤其是NLP和语言学的人，对他们进...\n",
       "82                                           马海祥博客点评：\n",
       "83  深度学习中的表征视角是非常有力的，也似乎能够解答为何深度神经网络如此有效，在此之上，我认为它...\n",
       "84  深度学习是个非常年轻的领域，理论根基还不强，观点也在快速地改变，我感觉神经网络中重视表征的这...\n",
       "85                    相关热词搜索：\\n自然语言\\n语言处理\\n单词嵌入\\n表征方法\n",
       "86            上一篇：九大排序算法的实现方法及算法分析\\n下一篇：详解内存数据库中的索引技术\n",
       "\n",
       "[87 rows x 1 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
